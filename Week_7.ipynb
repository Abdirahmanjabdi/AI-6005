{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQCSbqpwnCrM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af7a20b6"
      },
      "source": [
        "# Task\n",
        "Perform Naive Bayes classification for 'Accident Prediction', 'Weather-Based Game Prediction', and 'Loan Approval Prediction' using data from the file 's'. For Accident Prediction, classify an instance with attributes (Rain, Good, Normal, No). For Weather-Based Game Prediction, classify instances (sunny, hot, high, false) and (overcast, cool, high, true). For Loan Approval Prediction, calculate probabilities for (Employed, Good, Medium) and (Unemployed, Bad, Low). Discuss the impact of a scoring system on Naive Bayes probability calculations, and finally, summarize the findings and classifications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6580cccc"
      },
      "source": [
        "## Load and Prepare Datasets\n",
        "\n",
        "### Subtask:\n",
        "Load the necessary datasets for 'Accident Prediction', 'Weather-Based Game Prediction', and 'Loan Approval Prediction' from the provided file 's'. This step will parse the data into appropriate structures (e.g., pandas DataFrames) and preprocess categorical features as needed for Naive Bayes classification. If the datasets are not clearly separated, the plan will assume a method to identify them within the 's' file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f2d7c09"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the variable 's' is not defined in the current context, and the task requires loading data for three specific prediction tasks, I will begin by listing the CSV files available in the `/content/` directory. This will help identify potential data sources for 'Accident Prediction', 'Weather-Based Game Prediction', and 'Loan Approval Prediction'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8757d90b",
        "outputId": "65b38ac1-1c35-42c2-ac8d-64afa862cfde"
      },
      "source": [
        "import os\n",
        "\n",
        "# List all files in the /content/ directory to identify potential datasets\n",
        "file_list = os.listdir('/content/')\n",
        "print(\"Files available in /content/:\")\n",
        "for file_name in file_list:\n",
        "    print(file_name)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files available in /content/:\n",
            ".config\n",
            "dataset3.csv\n",
            "dataset4.csv\n",
            "dataset1.csv\n",
            "dataset2.csv\n",
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32626fda"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the files in `/content/` have been listed, I will load each of the identified CSV files (`dataset1.csv`, `dataset2.csv`, `dataset3.csv`, `dataset4.csv`) into pandas DataFrames and display their first few rows. This will help in understanding their content and mapping them to the specific prediction tasks: 'Accident Prediction', 'Weather-Based Game Prediction', and 'Loan Approval Prediction'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62e1c754",
        "outputId": "21890708-effa-4969-e1b0-ce095a9e5839"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_dataset1 = pd.read_csv('/content/dataset1.csv')\n",
        "df_dataset2 = pd.read_csv('/content/dataset2.csv')\n",
        "df_dataset3 = pd.read_csv('/content/dataset3.csv')\n",
        "df_dataset4 = pd.read_csv('/content/dataset4.csv')\n",
        "\n",
        "print(\"\\n--- Dataset 1 ---\")\n",
        "print(df_dataset1.head())\n",
        "print(\"\\n--- Dataset 2 ---\")\n",
        "print(df_dataset2.head())\n",
        "print(\"\\n--- Dataset 3 ---\")\n",
        "print(df_dataset3.head())\n",
        "print(\"\\n--- Dataset 4 ---\")\n",
        "print(df_dataset4.head())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Dataset 1 ---\n",
            "   SNo. Weather condition Road condition Traffic condition Engine problem  \\\n",
            "0     1              Rain            bad              high             no   \n",
            "1     2              snow        average            normal            yes   \n",
            "2     3             clear            bad             light             no   \n",
            "3     4             clear           good             light            yes   \n",
            "4     5              snow           good            normal             no   \n",
            "\n",
            "  Accident  \n",
            "0      yes  \n",
            "1      yes  \n",
            "2       no  \n",
            "3      yes  \n",
            "4       no  \n",
            "\n",
            "--- Dataset 2 ---\n",
            "    outlook temperature humidity  windy play\n",
            "0     sunny         hot     high  False   no\n",
            "1     sunny         hot     high   True   no\n",
            "2  overcast         hot     high  False  yes\n",
            "3     rainy        mild     high  False  yes\n",
            "4     rainy        cool   normal  False  yes\n",
            "\n",
            "--- Dataset 3 ---\n",
            "  EmploymentStatus CreditHistory IncomeLevel LoanApproved\n",
            "0         Employed          Good        High          Yes\n",
            "1       Unemployed           Bad         Low           No\n",
            "2         Employed          Good      Medium          Yes\n",
            "3         Employed           Bad      Medium           No\n",
            "4       Unemployed          Good         Low          Yes\n",
            "\n",
            "--- Dataset 4 ---\n",
            "                                           Dataset 4\n",
            "0  Fever | Cough | Fatigue | TravelHistory | Dise...\n",
            "1                           Yes Yes Yes Yes Positive\n",
            "2                              No Yes No No Negative\n",
            "3                             Yes No Yes No Positive\n",
            "4                             No Yes No Yes Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8921264f"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the previous data loading and inspection, I have identified the relevant datasets for each prediction task. Now, I will rename the dataframes for clarity, convert categorical columns to the appropriate 'category' dtype, remove unnecessary columns like 'SNo.', and then display the information and head of each processed DataFrame to confirm their structure and readiness for Naive Bayes classification. Dataset 4 is irrelevant to the stated tasks and will not be processed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57f67d3a",
        "outputId": "317102aa-6b20-4010-ada5-0cb6364cbaf9"
      },
      "source": [
        "df_accident_prediction = df_dataset1.copy()\n",
        "df_weather_game_prediction = df_dataset2.copy()\n",
        "df_loan_approval_prediction = df_dataset3.copy()\n",
        "\n",
        "# Preprocess df_accident_prediction\n",
        "df_accident_prediction = df_accident_prediction.drop(columns=['SNo.'])\n",
        "for col in df_accident_prediction.columns:\n",
        "    df_accident_prediction[col] = df_accident_prediction[col].astype('category')\n",
        "\n",
        "# Preprocess df_weather_game_prediction\n",
        "for col in df_weather_game_prediction.columns:\n",
        "    df_weather_game_prediction[col] = df_weather_game_prediction[col].astype('category')\n",
        "\n",
        "# Preprocess df_loan_approval_prediction\n",
        "for col in df_loan_approval_prediction.columns:\n",
        "    df_loan_approval_prediction[col] = df_loan_approval_prediction[col].astype('category')\n",
        "\n",
        "print(\"\\n--- Accident Prediction Dataset (df_accident_prediction) ---\")\n",
        "print(df_accident_prediction.info())\n",
        "print(df_accident_prediction.head())\n",
        "\n",
        "print(\"\\n--- Weather-Based Game Prediction Dataset (df_weather_game_prediction) ---\")\n",
        "print(df_weather_game_prediction.info())\n",
        "print(df_weather_game_prediction.head())\n",
        "\n",
        "print(\"\\n--- Loan Approval Prediction Dataset (df_loan_approval_prediction) ---\")\n",
        "print(df_loan_approval_prediction.info())\n",
        "print(df_loan_approval_prediction.head())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Accident Prediction Dataset (df_accident_prediction) ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10 entries, 0 to 9\n",
            "Data columns (total 5 columns):\n",
            " #   Column             Non-Null Count  Dtype   \n",
            "---  ------             --------------  -----   \n",
            " 0   Weather condition  10 non-null     category\n",
            " 1   Road condition     10 non-null     category\n",
            " 2   Traffic condition  10 non-null     category\n",
            " 3   Engine problem     10 non-null     category\n",
            " 4   Accident           10 non-null     category\n",
            "dtypes: category(5)\n",
            "memory usage: 898.0 bytes\n",
            "None\n",
            "  Weather condition Road condition Traffic condition Engine problem Accident\n",
            "0              Rain            bad              high             no      yes\n",
            "1              snow        average            normal            yes      yes\n",
            "2             clear            bad             light             no       no\n",
            "3             clear           good             light            yes      yes\n",
            "4              snow           good            normal             no       no\n",
            "\n",
            "--- Weather-Based Game Prediction Dataset (df_weather_game_prediction) ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14 entries, 0 to 13\n",
            "Data columns (total 5 columns):\n",
            " #   Column       Non-Null Count  Dtype   \n",
            "---  ------       --------------  -----   \n",
            " 0   outlook      14 non-null     category\n",
            " 1   temperature  14 non-null     category\n",
            " 2   humidity     14 non-null     category\n",
            " 3   windy        14 non-null     category\n",
            " 4   play         14 non-null     category\n",
            "dtypes: category(5)\n",
            "memory usage: 796.0 bytes\n",
            "None\n",
            "    outlook temperature humidity  windy play\n",
            "0     sunny         hot     high  False   no\n",
            "1     sunny         hot     high   True   no\n",
            "2  overcast         hot     high  False  yes\n",
            "3     rainy        mild     high  False  yes\n",
            "4     rainy        cool   normal  False  yes\n",
            "\n",
            "--- Loan Approval Prediction Dataset (df_loan_approval_prediction) ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5 entries, 0 to 4\n",
            "Data columns (total 4 columns):\n",
            " #   Column            Non-Null Count  Dtype   \n",
            "---  ------            --------------  -----   \n",
            " 0   EmploymentStatus  5 non-null      category\n",
            " 1   CreditHistory     5 non-null      category\n",
            " 2   IncomeLevel       5 non-null      category\n",
            " 3   LoanApproved      5 non-null      category\n",
            "dtypes: category(4)\n",
            "memory usage: 656.0 bytes\n",
            "None\n",
            "  EmploymentStatus CreditHistory IncomeLevel LoanApproved\n",
            "0         Employed          Good        High          Yes\n",
            "1       Unemployed           Bad         Low           No\n",
            "2         Employed          Good      Medium          Yes\n",
            "3         Employed           Bad      Medium           No\n",
            "4       Unemployed          Good         Low          Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab6a517b"
      },
      "source": [
        "## Task 1: Accident Prediction Analysis\n",
        "\n",
        "### Subtask:\n",
        "Perform Naive Bayes classification for the 'Accident Prediction' task, including calculating prior and conditional probabilities, and classifying a given instance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "196e6408"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define a `naive_bayes_classifier` function to perform Naive Bayes classification from scratch. This function will calculate prior and conditional probabilities from the training data and then use these to predict the class for a new instance by computing posterior probabilities. Finally, it will print the results and the classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c839960",
        "outputId": "9077be4c-3d78-4b20-e335-2ed2c4b67dc3"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def naive_bayes_classifier(df, target_column, new_instance):\n",
        "    \"\"\"\n",
        "    Performs Naive Bayes classification.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The training DataFrame.\n",
        "        target_column (str): The name of the target column.\n",
        "        new_instance (dict): A dictionary representing the instance to classify.\n",
        "\n",
        "    Returns:\n",
        "        tuple: The predicted class and a dictionary of posterior probabilities for each class.\n",
        "    \"\"\"\n",
        "    classes = df[target_column].unique()\n",
        "    prior_probabilities = {}\n",
        "    conditional_probabilities = {}\n",
        "    smoothed_factor = 1 # Laplace smoothing factor\n",
        "\n",
        "    # Calculate prior probabilities\n",
        "    for cls in classes:\n",
        "        prior_probabilities[cls] = (df[target_column] == cls).sum() / len(df)\n",
        "\n",
        "    # Calculate conditional probabilities with Laplace smoothing\n",
        "    features = [col for col in df.columns if col != target_column]\n",
        "    for cls in classes:\n",
        "        conditional_probabilities[cls] = {}\n",
        "        df_cls = df[df[target_column] == cls]\n",
        "        for feature in features:\n",
        "            conditional_probabilities[cls][feature] = {}\n",
        "            for value in df[feature].unique():\n",
        "                # Count occurrences of (feature_value, class) and total occurrences of class\n",
        "                count_feature_in_cls = ((df_cls[feature] == value).sum() + smoothed_factor)\n",
        "                count_cls = (len(df_cls) + smoothed_factor * len(df[feature].unique()))\n",
        "                conditional_probabilities[cls][feature][value] = count_feature_in_cls / count_cls\n",
        "\n",
        "    # Calculate posterior probabilities for the new instance\n",
        "    posterior_probabilities = {cls: np.log(prior_probabilities[cls]) for cls in classes}\n",
        "\n",
        "    for cls in classes:\n",
        "        for feature, value in new_instance.items():\n",
        "            if feature in features:\n",
        "                if value in conditional_probabilities[cls][feature]:\n",
        "                    posterior_probabilities[cls] += np.log(conditional_probabilities[cls][feature][value])\n",
        "                else:\n",
        "                    # Handle unseen feature values in test instance (using smoothing already applied)\n",
        "                    # This part assumes smoothing is enough, or we might need to handle new values explicitly\n",
        "                    # For simplicity, if value not seen in training for a class, its probability is very low\n",
        "                    # which log-transformed will be a large negative number.\n",
        "                    # Here we'll use a very small probability if a specific value for a feature is not seen for a class\n",
        "                    posterior_probabilities[cls] += np.log(smoothed_factor / (len(df_cls) + smoothed_factor * len(df[feature].unique())))\n",
        "\n",
        "    # Convert log probabilities back to actual probabilities (optional, but good for understanding)\n",
        "    # To avoid numerical underflow, it's common to work with log probabilities and then normalize\n",
        "    max_log_prob = max(posterior_probabilities.values())\n",
        "    exp_probs = {cls: np.exp(log_prob - max_log_prob) for cls, log_prob in posterior_probabilities.items()}\n",
        "    sum_exp_probs = sum(exp_probs.values())\n",
        "    normalized_posterior_probabilities = {cls: exp_prob / sum_exp_probs for cls, exp_prob in exp_probs.items()}\n",
        "\n",
        "    # Predict the class\n",
        "    predicted_class = max(normalized_posterior_probabilities, key=normalized_posterior_probabilities.get)\n",
        "\n",
        "    return predicted_class, normalized_posterior_probabilities\n",
        "\n",
        "# Instance to classify for Accident Prediction\n",
        "accident_instance = {\n",
        "    'Weather condition': 'Rain',\n",
        "    'Road condition': 'Good',\n",
        "    'Traffic condition': 'Normal',\n",
        "    'Engine problem': 'No'\n",
        "}\n",
        "\n",
        "# Perform classification\n",
        "predicted_accident_class, accident_probabilities = naive_bayes_classifier(\n",
        "    df=df_accident_prediction,\n",
        "    target_column='Accident',\n",
        "    new_instance=accident_instance\n",
        ")\n",
        "\n",
        "print(\"\\n--- Accident Prediction Classification ---\")\n",
        "print(f\"Instance to classify: {accident_instance}\")\n",
        "print(f\"Posterior probability for Accident=yes: {accident_probabilities.get('yes', 0):.4f}\")\n",
        "print(f\"Posterior probability for Accident=no: {accident_probabilities.get('no', 0):.4f}\")\n",
        "print(f\"Predicted Accident Class: {predicted_accident_class}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Accident Prediction Classification ---\n",
            "Instance to classify: {'Weather condition': 'Rain', 'Road condition': 'Good', 'Traffic condition': 'Normal', 'Engine problem': 'No'}\n",
            "Posterior probability for Accident=yes: 0.6667\n",
            "Posterior probability for Accident=no: 0.3333\n",
            "Predicted Accident Class: yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2314035c"
      },
      "source": [
        "## Task 2: Weather-Based Game Prediction Analysis\n",
        "\n",
        "### Subtask:\n",
        "Conduct Naive Bayes classification for the 'Weather-Based Game Prediction' task. Calculate prior and conditional probabilities, classify two given instances, and compare the classification for the first instance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "167d7248"
      },
      "source": [
        "**Reasoning**:\n",
        "I will classify the first instance for Weather-Based Game Prediction using the previously defined `naive_bayes_classifier` function, as instructed by the subtask, and then print the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "947fb583",
        "outputId": "5254e57c-c598-4b37-cc6f-5b10664a01f6"
      },
      "source": [
        "game_instance_1 = {\n",
        "    'outlook': 'sunny',\n",
        "    'temperature': 'hot',\n",
        "    'humidity': 'high',\n",
        "    'windy': False\n",
        "}\n",
        "\n",
        "# Perform classification for game_instance_1\n",
        "predicted_game_class_1, game_probabilities_1 = naive_bayes_classifier(\n",
        "    df=df_weather_game_prediction,\n",
        "    target_column='play',\n",
        "    new_instance=game_instance_1\n",
        ")\n",
        "\n",
        "print(\"\\n--- Weather-Based Game Prediction Classification (Instance 1) ---\")\n",
        "print(f\"Instance to classify: {game_instance_1}\")\n",
        "print(f\"Posterior probability for play=yes: {game_probabilities_1.get('yes', 0):.4f}\")\n",
        "print(f\"Posterior probability for play=no: {game_probabilities_1.get('no', 0):.4f}\")\n",
        "print(f\"Predicted Game Class: {predicted_game_class_1}\")\n",
        "\n",
        "game_instance_2 = {\n",
        "    'outlook': 'overcast',\n",
        "    'temperature': 'cool',\n",
        "    'humidity': 'high',\n",
        "    'windy': True\n",
        "}\n",
        "\n",
        "# Perform classification for game_instance_2\n",
        "predicted_game_class_2, game_probabilities_2 = naive_bayes_classifier(\n",
        "    df=df_weather_game_prediction,\n",
        "    target_column='play',\n",
        "    new_instance=game_instance_2\n",
        ")\n",
        "\n",
        "print(\"\\n--- Weather-Based Game Prediction Classification (Instance 2) ---\")\n",
        "print(f\"Instance to classify: {game_instance_2}\")\n",
        "print(f\"Posterior probability for play=yes: {game_probabilities_2.get('yes', 0):.4f}\")\n",
        "print(f\"Posterior probability for play=no: {game_probabilities_2.get('no', 0):.4f}\")\n",
        "print(f\"Predicted Game Class: {predicted_game_class_2}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Weather-Based Game Prediction Classification (Instance 1) ---\n",
            "Instance to classify: {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': False}\n",
            "Posterior probability for play=yes: 0.3120\n",
            "Posterior probability for play=no: 0.6880\n",
            "Predicted Game Class: no\n",
            "\n",
            "--- Weather-Based Game Prediction Classification (Instance 2) ---\n",
            "Instance to classify: {'outlook': 'overcast', 'temperature': 'cool', 'humidity': 'high', 'windy': True}\n",
            "Posterior probability for play=yes: 0.7216\n",
            "Posterior probability for play=no: 0.2784\n",
            "Predicted Game Class: yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c8f4e5c"
      },
      "source": [
        "## Task 3: Loan Approval Prediction Analysis\n",
        "\n",
        "### Subtask:\n",
        "Execute Naive Bayes classification for the 'Loan Approval Prediction' task. Calculate prior and conditional probabilities for 'LoanApproved' (Yes/No) based on 'EmploymentStatus', 'CreditHistory', and 'IncomeLevel'. Calculate P(LoanApproved = Yes | EmploymentStatus = Employed, CreditHistory = Good, IncomeLevel = Medium) and P(LoanApproved = No | EmploymentStatus = Unemployed, CreditHistory = Bad, IncomeLevel = Low).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2d96716"
      },
      "source": [
        "**Reasoning**:\n",
        "I will classify two new instances for Loan Approval Prediction using the previously defined `naive_bayes_classifier` function, as instructed by the subtask, and then print the results for each instance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44d2d10a",
        "outputId": "e8232dc9-2294-48bb-9fc2-ad03822cb9b8"
      },
      "source": [
        "loan_instance_1 = {\n",
        "    'EmploymentStatus': 'Employed',\n",
        "    'CreditHistory': 'Good',\n",
        "    'IncomeLevel': 'Medium'\n",
        "}\n",
        "\n",
        "# Perform classification for loan_instance_1\n",
        "predicted_loan_class_1, loan_probabilities_1 = naive_bayes_classifier(\n",
        "    df=df_loan_approval_prediction,\n",
        "    target_column='LoanApproved',\n",
        "    new_instance=loan_instance_1\n",
        ")\n",
        "\n",
        "print(\"\\n--- Loan Approval Prediction Classification (Instance 1) ---\")\n",
        "print(f\"Instance to classify: {loan_instance_1}\")\n",
        "print(f\"Posterior probability for LoanApproved=Yes: {loan_probabilities_1.get('Yes', 0):.4f}\")\n",
        "print(f\"Posterior probability for LoanApproved=No: {loan_probabilities_1.get('No', 0):.4f}\")\n",
        "print(f\"Predicted Loan Approval Class: {predicted_loan_class_1}\")\n",
        "\n",
        "loan_instance_2 = {\n",
        "    'EmploymentStatus': 'Unemployed',\n",
        "    'CreditHistory': 'Bad',\n",
        "    'IncomeLevel': 'Low'\n",
        "}\n",
        "\n",
        "# Perform classification for loan_instance_2\n",
        "predicted_loan_class_2, loan_probabilities_2 = naive_bayes_classifier(\n",
        "    df=df_loan_approval_prediction,\n",
        "    target_column='LoanApproved',\n",
        "    new_instance=loan_instance_2\n",
        ")\n",
        "\n",
        "print(\"\\n--- Loan Approval Prediction Classification (Instance 2) ---\")\n",
        "print(f\"Instance to classify: {loan_instance_2}\")\n",
        "print(f\"Posterior probability for LoanApproved=Yes: {loan_probabilities_2.get('Yes', 0):.4f}\")\n",
        "print(f\"Posterior probability for LoanApproved=No: {loan_probabilities_2.get('No', 0):.4f}\")\n",
        "print(f\"Predicted Loan Approval Class: {predicted_loan_class_2}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Loan Approval Prediction Classification (Instance 1) ---\n",
            "Instance to classify: {'EmploymentStatus': 'Employed', 'CreditHistory': 'Good', 'IncomeLevel': 'Medium'}\n",
            "Posterior probability for LoanApproved=Yes: 0.8276\n",
            "Posterior probability for LoanApproved=No: 0.1724\n",
            "Predicted Loan Approval Class: Yes\n",
            "\n",
            "--- Loan Approval Prediction Classification (Instance 2) ---\n",
            "Instance to classify: {'EmploymentStatus': 'Unemployed', 'CreditHistory': 'Bad', 'IncomeLevel': 'Low'}\n",
            "Posterior probability for LoanApproved=Yes: 0.2105\n",
            "Posterior probability for LoanApproved=No: 0.7895\n",
            "Predicted Loan Approval Class: No\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6da57c4c"
      },
      "source": [
        "## Task 3 (Advanced): Scoring System Impact Discussion\n",
        "\n",
        "### Subtask:\n",
        "Discuss the potential impact of introducing a scoring system for applicants (e.g., 'Employed' score of 3, 'Unemployed' score of 1) on Naive Bayes probability calculations. Explain how such a system might be integrated or how it might alter the interpretation of features in a Naive Bayes model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "739e854a"
      },
      "source": [
        "## Task 3 (Advanced): Scoring System Impact Discussion\n",
        "\n",
        "### Subtask:\n",
        "Discuss the potential impact of introducing a scoring system for applicants (e.g., 'Employed' score of 3, 'Unemployed' score of 1) on Naive Bayes probability calculations. Explain how such a system might be integrated or how it might alter the interpretation of features in a Naive Bayes model.\n",
        "\n",
        "#### Instructions\n",
        "1. Explain how a scoring system (e.g., converting categorical features like 'EmploymentStatus' into numerical scores) would affect the Naive Bayes model's input data.\n",
        "2. Discuss whether the current `naive_bayes_classifier` function, which is designed for categorical features, would need modifications to handle numerical scores directly.\n",
        "3. Describe potential ways to integrate such a scoring system into a Naive Bayes approach. Consider whether the scores could be treated as discrete categories or if a different type of Naive Bayes (e.g., Gaussian Naive Bayes if scores are continuous and follow a normal distribution) would be more appropriate.\n",
        "4. Explain how this integration might alter the calculation of conditional probabilities and the overall interpretation of feature importance within the Naive Bayes framework.\n",
        "5. Discuss the advantages and disadvantages of using a scoring system compared to treating categorical features directly in a Naive Bayes model for the 'Loan Approval Prediction' task.\n",
        "\n",
        "---\n",
        "\n",
        "### Impact of a Scoring System on Naive Bayes\n",
        "\n",
        "1.  **Effect on Naive Bayes Model's Input Data:**\n",
        "    Introducing a scoring system, where categorical features like 'EmploymentStatus' ('Employed', 'Unemployed') are converted into numerical scores (e.g., 'Employed' = 3, 'Unemployed' = 1), fundamentally changes the nature of the input data for the Naive Bayes model. Instead of dealing with discrete, non-ordered categories, the model would receive numerical values that imply an order or magnitude. This transformation shifts the data from purely nominal to ordinal or even interval, depending on how the scores are designed and interpreted. The Naive Bayes classifier, in its standard form for categorical data, expects discrete categories, not numerical magnitudes.\n",
        "\n",
        "2.  **Modifications to `naive_bayes_classifier` Function:**\n",
        "    The current `naive_bayes_classifier` function is explicitly designed for categorical features. It calculates conditional probabilities by counting occurrences of each *category* within each class. If numerical scores are introduced, the function would require significant modification:\n",
        "    *   **Direct Use (as categories):** If the scores (e.g., 1, 2, 3) are treated as new categorical values, the function could still work without structural changes. However, this would disregard the numerical ordering inherent in the scores, treating '1', '2', and '3' as distinct, unordered labels, similar to 'red', 'green', 'blue'. This approach might lose valuable information from the scoring system.\n",
        "    *   **Numerical Handling:** To leverage the numerical nature of scores, the function would need to incorporate a different probability distribution. For instance, if the scores are treated as discrete integers that are ordered, one might still use a categorical approach but acknowledge the ordering. More commonly, if scores are continuous or treated as such, a different variant of Naive Bayes, like **Gaussian Naive Bayes**, would be necessary. This would involve calculating the mean and standard deviation of scores for each feature and class, assuming a Gaussian (normal) distribution for these numerical features.\n",
        "\n",
        "3.  **Integration Methods into Naive Bayes:**\n",
        "    *   **Treat as Discrete Categories:** The simplest integration is to treat the numerical scores themselves as discrete categorical values. For example, if 'EmploymentStatus' becomes 'Score_Employment' with values 1 and 3, the model would calculate `P(Score_Employment=1 | LoanApproved=Yes)` and `P(Score_Employment=3 | LoanApproved=Yes)` etc., just as it would for any other category. This is straightforward but ignores the numerical relationship.\n",
        "    *   **Discretization/Binning:** If the numerical scores have a wide range or represent a continuous spectrum (e.g., credit scores from 300-850), they could be binned into a few discrete categories (e.g., 'Low Score', 'Medium Score', 'High Score'). The categorical Naive Bayes classifier could then be applied to these bins.\n",
        "    *   **Gaussian Naive Bayes:** This is the most appropriate approach when the scoring system generates numerical values that are assumed to follow a continuous distribution, typically a Gaussian distribution. For each feature (score) and each class (e.g., `LoanApproved=Yes`), the model would estimate the mean and variance of the scores. The conditional probability `P(Score | Class)` would then be calculated using the probability density function of the Gaussian distribution.\n",
        "    *   **Multinomial/Bernoulli Naive Bayes:** These are generally used for count data or binary features, respectively. While scores could be interpreted as 'counts' in some abstract way or binarized, they are less intuitive fits than Gaussian or a modified categorical approach.\n",
        "\n",
        "4.  **Alteration of Conditional Probabilities and Feature Interpretation:**\n",
        "    *   **Conditional Probabilities:**\n",
        "        *   **Categorical Approach (with scores as categories):** The conditional probabilities `P(Feature=score | Class)` would still be calculated as frequencies. However, the 'meaning' of these frequencies would change. Instead of `P(Weather=Rain | Accident=yes)`, it would be `P(EmploymentScore=3 | LoanApproved=Yes)`. The direct interpretation is the frequency of observing that score given the class.\n",
        "        *   **Gaussian Naive Bayes Approach:** Conditional probabilities would no longer be simple counts. Instead, they would be derived from the probability density function (PDF) of a Gaussian distribution. For a given score `x`, the conditional probability would be proportional to `exp(- (x - mean)^2 / (2 * variance)) / sqrt(2 * pi * variance)`. This means that scores closer to the class's mean for that feature would yield higher probabilities.\n",
        "    *   **Feature Interpretation:**\n",
        "        *   With a simple categorical interpretation of scores, the interpretation of feature importance remains similar: certain score values are more indicative of a class than others. However, the model doesn't inherently understand that a score of 3 is 'better' than a score of 1; it just sees them as different labels.\n",
        "        *   With Gaussian Naive Bayes, the interpretation gains a quantitative dimension. Features with scores that are highly separated between classes (i.e., different means and/or small variances within classes) would be considered more important. The model can now distinguish between a 'good' score and a 'bad' score in a more numerically informed way, rather than just as distinct types.\n",
        "\n",
        "5.  **Advantages and Disadvantages for Loan Approval Prediction:**\n",
        "    **Advantages of using a Scoring System:**\n",
        "    *   **Incorporation of Domain Knowledge:** A scoring system allows domain experts to inject their knowledge about the relative importance or quality of different categorical levels. For instance, 'Employed' is definitively 'better' for a loan than 'Unemployed', and a score reflects this hierarchy.\n",
        "    *   **Potentially More Informative:** If scores genuinely reflect an underlying continuous or ordinal scale that is relevant to the target variable, they can provide more information to the model than treating categories as purely nominal.\n",
        "    *   **Compatibility with Gaussian Naive Bayes:** If scores are designed to be approximately continuous and normally distributed, they open the door to using Gaussian Naive Bayes, which might be more powerful for numerical data than simply treating numbers as discrete categories.\n",
        "\n",
        "    **Disadvantages of using a Scoring System:**\n",
        "    *   **Subjectivity and Bias:** The design of the scoring system itself can be subjective and introduce human bias. The choice of scores (e.g., why 3 for 'Employed' and 1 for 'Unemployed' instead of 10 and 0?) can significantly impact the model's performance and decisions.\n",
        "    *   **Loss of Information (if poorly designed):** If the scoring system oversimplifies complex relationships or assigns scores that don't accurately reflect the true underlying numerical relationship, it can lead to a loss of information compared to directly using categorical values.\n",
        "    *   **Increased Complexity (for interpretation):** While scores aim to simplify, their integration can make the model more complex to interpret if the chosen Naive Bayes variant (e.g., Gaussian) is less intuitive than simple frequency counting for categorical data.\n",
        "    *   **Assumptions of Gaussian Naive Bayes:** If Gaussian Naive Bayes is used, it assumes that the feature scores within each class are normally distributed. If this assumption is violated, the model's performance can suffer.\n",
        "    *   **Data Sparsity Issues:** For categories with very few data points, assigning scores might exacerbate sparsity problems or lead to unstable probability estimates, especially if not adequately smoothed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee241053"
      },
      "source": [
        "## Task 3 (Advanced): Scoring System Impact Discussion\n",
        "\n",
        "### Subtask:\n",
        "Discuss the potential impact of introducing a scoring system for applicants (e.g., 'Employed' score of 3, 'Unemployed' score of 1) on Naive Bayes probability calculations. Explain how such a system might be integrated or how it might alter the interpretation of features in a Naive Bayes model.\n",
        "\n",
        "#### Instructions\n",
        "1. Explain how a scoring system (e.g., converting categorical features like 'EmploymentStatus' into numerical scores) would affect the Naive Bayes model's input data.\n",
        "2. Discuss whether the current `naive_bayes_classifier` function, which is designed for categorical features, would need modifications to handle numerical scores directly.\n",
        "3. Describe potential ways to integrate such a scoring system into a Naive Bayes approach. Consider whether the scores could be treated as discrete categories or if a different type of Naive Bayes (e.g., Gaussian Naive Bayes if scores are continuous and follow a normal distribution) would be more appropriate.\n",
        "4. Explain how this integration might alter the calculation of conditional probabilities and the overall interpretation of feature importance within the Naive Bayes framework.\n",
        "5. Discuss the advantages and disadvantages of using a scoring system compared to treating categorical features directly in a Naive Bayes model for the 'Loan Approval Prediction' task.\n",
        "\n",
        "---\n",
        "\n",
        "### Impact of a Scoring System on Naive Bayes\n",
        "\n",
        "1.  **Effect on Naive Bayes Model's Input Data:**\n",
        "    Introducing a scoring system, where categorical features like 'EmploymentStatus' ('Employed', 'Unemployed') are converted into numerical scores (e.g., 'Employed' = 3, 'Unemployed' = 1), fundamentally changes the nature of the input data for the Naive Bayes model. Instead of dealing with discrete, non-ordered categories, the model would receive numerical values that imply an order or magnitude. This transformation shifts the data from purely nominal to ordinal or even interval, depending on how the scores are designed and interpreted. The Naive Bayes classifier, in its standard form for categorical data, expects discrete categories, not numerical magnitudes.\n",
        "\n",
        "2.  **Modifications to `naive_bayes_classifier` Function:**\n",
        "    The current `naive_bayes_classifier` function is explicitly designed for categorical features. It calculates conditional probabilities by counting occurrences of each *category* within each class. If numerical scores are introduced, the function would require significant modification:\n",
        "    *   **Direct Use (as categories):** If the scores (e.g., 1, 2, 3) are treated as new categorical values, the function could still work without structural changes. However, this would disregard the numerical ordering inherent in the scores, treating '1', '2', and '3' as distinct, unordered labels, similar to 'red', 'green', 'blue'. This approach might lose valuable information from the scoring system.\n",
        "    *   **Numerical Handling:** To leverage the numerical nature of scores, the function would need to incorporate a different probability distribution. For instance, if the scores are treated as discrete integers that are ordered, one might still use a categorical approach but acknowledge the ordering. More commonly, if scores are continuous or treated as such, a different variant of Naive Bayes, like **Gaussian Naive Bayes**, would be necessary. This would involve calculating the mean and standard deviation of scores for each feature and class, assuming a Gaussian (normal) distribution for these numerical features.\n",
        "\n",
        "3.  **Integration Methods into Naive Bayes:**\n",
        "    *   **Treat as Discrete Categories:** The simplest integration is to treat the numerical scores themselves as discrete categorical values. For example, if 'EmploymentStatus' becomes 'Score_Employment' with values 1 and 3, the model would calculate `P(Score_Employment=1 | LoanApproved=Yes)` and `P(Score_Employment=3 | LoanApproved=Yes)` etc., just as it would for any other category. This is straightforward but ignores the numerical relationship.\n",
        "    *   **Discretization/Binning:** If the numerical scores have a wide range or represent a continuous spectrum (e.g., credit scores from 300-850), they could be binned into a few discrete categories (e.g., 'Low Score', 'Medium Score', 'High Score'). The categorical Naive Bayes classifier could then be applied to these bins.\n",
        "    *   **Gaussian Naive Bayes:** This is the most appropriate approach when the scoring system generates numerical values that are assumed to follow a continuous distribution, typically a Gaussian distribution. For each feature (score) and each class (e.g., `LoanApproved=Yes`), the model would estimate the mean and variance of the scores. The conditional probability `P(Score | Class)` would then be calculated using the probability density function of the Gaussian distribution.\n",
        "    *   **Multinomial/Bernoulli Naive Bayes:** These are generally used for count data or binary features, respectively. While scores could be interpreted as 'counts' in some abstract way or binarized, they are less intuitive fits than Gaussian or a modified categorical approach.\n",
        "\n",
        "4.  **Alteration of Conditional Probabilities and Feature Interpretation:**\n",
        "    *   **Conditional Probabilities:**\n",
        "        *   **Categorical Approach (with scores as categories):** The conditional probabilities `P(Feature=score | Class)` would still be calculated as frequencies. However, the 'meaning' of these frequencies would change. Instead of `P(Weather=Rain | Accident=yes)`, it would be `P(EmploymentScore=3 | LoanApproved=Yes)`. The direct interpretation is the frequency of observing that score given the class.\n",
        "        *   **Gaussian Naive Bayes Approach:** Conditional probabilities would no longer be simple counts. Instead, they would be derived from the probability density function (PDF) of a Gaussian distribution. For a given score `x`, the conditional probability would be proportional to `exp(- (x - mean)^2 / (2 * variance)) / sqrt(2 * pi * variance)`. This means that scores closer to the class's mean for that feature would yield higher probabilities.\n",
        "    *   **Feature Interpretation:**\n",
        "        *   With a simple categorical interpretation of scores, the interpretation of feature importance remains similar: certain score values are more indicative of a class than others. However, the model doesn't inherently understand that a score of 3 is 'better' than a score of 1; it just sees them as different labels.\n",
        "        *   With Gaussian Naive Bayes, the interpretation gains a quantitative dimension. Features with scores that are highly separated between classes (i.e., different means and/or small variances within classes) would be considered more important. The model can now distinguish between a 'good' score and a 'bad' score in a more numerically informed way, rather than just as distinct types.\n",
        "\n",
        "5.  **Advantages and Disadvantages for Loan Approval Prediction:**\n",
        "    **Advantages of using a Scoring System:**\n",
        "    *   **Incorporation of Domain Knowledge:** A scoring system allows domain experts to inject their knowledge about the relative importance or quality of different categorical levels. For instance, 'Employed' is definitively 'better' for a loan than 'Unemployed', and a score reflects this hierarchy.\n",
        "    *   **Potentially More Informative:** If scores genuinely reflect an underlying continuous or ordinal scale that is relevant to the target variable, they can provide more information to the model than treating categories as purely nominal.\n",
        "    *   **Compatibility with Gaussian Naive Bayes:** If scores are designed to be approximately continuous and normally distributed, they open the door to using Gaussian Naive Bayes, which might be more powerful for numerical data than simply treating numbers as discrete categories.\n",
        "\n",
        "    **Disadvantages of using a Scoring System:**\n",
        "    *   **Subjectivity and Bias:** The design of the scoring system itself can be subjective and introduce human bias. The choice of scores (e.g., why 3 for 'Employed' and 1 for 'Unemployed' instead of 10 and 0?) can significantly impact the model's performance and decisions.\n",
        "    *   **Loss of Information (if poorly designed):** If the scoring system oversimplifies complex relationships or assigns scores that don't accurately reflect the true underlying numerical relationship, it can lead to a loss of information compared to directly using categorical values.\n",
        "    *   **Increased Complexity (for interpretation):** While scores aim to simplify, their integration can make the model more complex to interpret if the chosen Naive Bayes variant (e.g., Gaussian) is less intuitive than simple frequency counting for categorical data.\n",
        "    *   **Assumptions of Gaussian Naive Bayes:** If Gaussian Naive Bayes is used, it assumes that the feature scores within each class are normally distributed. If this assumption is violated, the model's performance can suffer.\n",
        "    *   **Data Sparsity Issues:** For categories with very few data points, assigning scores might exacerbate sparsity problems or lead to unstable probability estimates, especially if not adequately smoothed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16df8759"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the key findings, calculated probabilities, and classifications for all three tasks. This will include the predictions for the specific instances in each task and the discussion on the scoring system's impact.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4e189ec"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "\n",
        "*   **For Accident Prediction, classify an instance with attributes (Rain, Good, Normal, No).**\n",
        "    The instance `{'Weather condition': 'Rain', 'Road condition': 'Good', 'Traffic condition': 'Normal', 'Engine problem': 'No'}` is predicted to result in an accident (`yes`) with a posterior probability of 0.6667.\n",
        "*   **For Weather-Based Game Prediction, classify instances (sunny, hot, high, false) and (overcast, cool, high, true).**\n",
        "    *   The instance `{'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': False}` is predicted not to play (`no`) with a posterior probability of 0.6880.\n",
        "    *   The instance `{'outlook': 'overcast', 'temperature': 'cool', 'humidity': 'high', 'windy': True}` is predicted to play (`yes`) with a posterior probability of 0.7216.\n",
        "*   **For Loan Approval Prediction, calculate probabilities for (Employed, Good, Medium) and (Unemployed, Bad, Low).**\n",
        "    *   For `{'EmploymentStatus': 'Employed', 'CreditHistory': 'Good', 'IncomeLevel': 'Medium'}`, the probability of `LoanApproved=Yes` is 0.8276, and `LoanApproved=No` is 0.1724. The predicted class is 'Yes'.\n",
        "    *   For `{'EmploymentStatus': 'Unemployed', 'CreditHistory': 'Bad', 'IncomeLevel': 'Low'}`, the probability of `LoanApproved=Yes` is 0.2105, and `LoanApproved=No` is 0.7895. The predicted class is 'No'.\n",
        "*   **Discuss the impact of a scoring system on Naive Bayes probability calculations.**\n",
        "    Introducing a scoring system converts categorical features into numerical values, changing the model's input data from nominal to ordinal or interval. This necessitates modifications to a standard categorical Naive Bayes classifier, potentially requiring the use of Gaussian Naive Bayes if scores are treated as continuous. This alters conditional probability calculations from simple frequency counts to probability density functions, offering advantages like incorporating domain knowledge but also disadvantages such as subjectivity in score design and reliance on distributional assumptions.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Three distinct datasets were identified and preprocessed for Naive Bayes classification: 'Accident Prediction' (10 entries, 5 categorical columns), 'Weather-Based Game Prediction' (14 entries, 5 categorical columns), and 'Loan Approval Prediction' (5 entries, 4 categorical columns).\n",
        "*   For the 'Accident Prediction' task, an instance with `Rain`, `Good`, `Normal`, and `No Engine Problem` attributes was classified as `yes` for an accident with a posterior probability of 0.6667.\n",
        "*   In 'Weather-Based Game Prediction', an instance with `sunny`, `hot`, `high humidity`, and `not windy` was classified as `no` for playing with a probability of 0.6880, while an `overcast`, `cool`, `high humidity`, and `windy` instance was classified as `yes` for playing with a probability of 0.7216.\n",
        "*   For 'Loan Approval Prediction', an employed individual with good credit and medium income was classified as `Yes` for loan approval (0.8276 probability), whereas an unemployed individual with bad credit and low income was classified as `No` for loan approval (0.7895 probability).\n",
        "*   The discussion on scoring systems highlighted that converting categorical features to numerical scores (e.g., 'Employed' = 3, 'Unemployed' = 1) transforms the data type, requiring adjustments to the Naive Bayes model. While this can integrate domain knowledge and potentially offer more informative feature interpretation via methods like Gaussian Naive Bayes, it introduces subjectivity in score design and relies on specific distributional assumptions.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The consistent application of the custom `naive_bayes_classifier` function across diverse datasets demonstrates its robustness for categorical data, which could be further enhanced by supporting other Naive Bayes variants (e.g., Gaussian) to handle numerical features directly.\n",
        "*   For future predictive modeling, carefully consider the trade-offs between directly using categorical features and introducing a scoring system; while scoring can inject valuable domain expertise, it's crucial to validate the scoring logic to avoid introducing bias or misrepresenting underlying relationships.\n"
      ]
    }
  ]
}